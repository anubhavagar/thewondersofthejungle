{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gymnastics Apparatus Detection - MediaPipe Model Maker Training\n",
                "\n",
                "This notebook trains a custom MediaPipe object detection model for gymnastics apparatus.\n",
                "\n",
                "**Dataset**: 5,220 images in Pascal VOC format  \n",
                "**Classes**: Balance_Beam, Horizontal_Bar, Parallel_Bars, Pommel_Horse, Still_Rings, Uneven_Bars, Vault  \n",
                "**Model**: EfficientDet-Lite2 (balanced speed/accuracy)  \n",
                "**Output**: TFLite model for MediaPipe integration\n",
                "\n",
                "---\n",
                "\n",
                "## üìã Instructions\n",
                "\n",
                "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
                "2. **Upload Dataset**: Upload your `raw_object_detect_pascalvoc` folder to Colab\n",
                "3. **Run All Cells**: Runtime ‚Üí Run all\n",
                "4. **Download Model**: Download the trained `.tflite` file at the end"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q mediapipe-model-maker"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import xml.etree.ElementTree as ET\n",
                "from pathlib import Path\n",
                "import shutil\n",
                "from sklearn.model_selection import train_test_split\n",
                "import cv2\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "from datetime import datetime\n",
                "from mediapipe_model_maker import object_detector\n",
                "import tensorflow as tf\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully\")\n",
                "print(f\"TensorFlow version: {tf.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Upload Dataset\n",
                "\n",
                "**Option A**: Upload ZIP file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "import zipfile\n",
                "\n",
                "# Upload dataset ZIP\n",
                "print(\"üì§ Upload your dataset ZIP file (raw_object_detect_pascalvoc.zip)\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Extract\n",
                "for filename in uploaded.keys():\n",
                "    print(f\"Extracting {filename}...\")\n",
                "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
                "        zip_ref.extractall('/content')\n",
                "\n",
                "print(\"‚úÖ Dataset uploaded and extracted\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Option B**: Mount Google Drive (if dataset is in Drive)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from google.colab import drive\n",
                "# drive.mount('/content/drive')\n",
                "# \n",
                "# # Update this path to your dataset location in Drive\n",
                "# RAW_DATA_DIR = Path('/content/drive/MyDrive/gym_data/raw_object_detect_pascalvoc')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths\n",
                "RAW_DATA_DIR = Path('/content/raw_object_detect_pascalvoc')  # Update if needed\n",
                "OUTPUT_DIR = Path('/content/mediapipe_dataset')\n",
                "MODEL_OUTPUT_DIR = Path('/content/trained_model')\n",
                "\n",
                "# Training parameters\n",
                "EPOCHS = 50\n",
                "BATCH_SIZE = 8\n",
                "\n",
                "# Data split\n",
                "TRAIN_SPLIT = 0.7\n",
                "VAL_SPLIT = 0.2\n",
                "TEST_SPLIT = 0.1\n",
                "\n",
                "# Classes (alphabetically sorted)\n",
                "CLASSES = [\n",
                "    \"Balance_Beam\",\n",
                "    \"Horizontal_Bar\",\n",
                "    \"Parallel_Bars\",\n",
                "    \"Pommel_Horse\",\n",
                "    \"Still_Rings\",\n",
                "    \"Uneven_Bars\",\n",
                "    \"Vault\"\n",
                "]\n",
                "\n",
                "# Model architecture\n",
                "MODEL_SPEC = \"efficientdet_lite2\"  # Options: lite0, lite2, lite4\n",
                "\n",
                "print(f\"‚úÖ Configuration set\")\n",
                "print(f\"   Model: {MODEL_SPEC}\")\n",
                "print(f\"   Classes: {len(CLASSES)}\")\n",
                "print(f\"   Epochs: {EPOCHS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Data Preparation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def parse_pascal_voc(xml_file):\n",
                "    \"\"\"Parse Pascal VOC XML annotation.\"\"\"\n",
                "    tree = ET.parse(xml_file)\n",
                "    root = tree.getroot()\n",
                "    \n",
                "    size = root.find('size')\n",
                "    width = int(size.find('width').text)\n",
                "    height = int(size.find('height').text)\n",
                "    filename = root.find('filename').text\n",
                "    \n",
                "    objects = []\n",
                "    for obj in root.findall('object'):\n",
                "        name = obj.find('name').text\n",
                "        bbox = obj.find('bndbox')\n",
                "        \n",
                "        xmin = int(bbox.find('xmin').text)\n",
                "        ymin = int(bbox.find('ymin').text)\n",
                "        xmax = int(bbox.find('xmax').text)\n",
                "        ymax = int(bbox.find('ymax').text)\n",
                "        \n",
                "        objects.append({\n",
                "            'class': name,\n",
                "            'bbox': [xmin, ymin, xmax, ymax]\n",
                "        })\n",
                "    \n",
                "    return {\n",
                "        'filename': filename,\n",
                "        'width': width,\n",
                "        'height': height,\n",
                "        'objects': objects\n",
                "    }\n",
                "\n",
                "\n",
                "def convert_to_coco_format(annotations_list, class_mapping):\n",
                "    \"\"\"Convert Pascal VOC to COCO format.\"\"\"\n",
                "    coco_data = {\n",
                "        \"images\": [],\n",
                "        \"annotations\": [],\n",
                "        \"categories\": []\n",
                "    }\n",
                "    \n",
                "    # Create categories\n",
                "    for class_name, class_id in class_mapping.items():\n",
                "        coco_data[\"categories\"].append({\n",
                "            \"id\": class_id + 1,\n",
                "            \"name\": class_name,\n",
                "            \"supercategory\": \"apparatus\"\n",
                "        })\n",
                "    \n",
                "    annotation_id = 1\n",
                "    \n",
                "    for image_id, annotation in enumerate(annotations_list, start=1):\n",
                "        coco_data[\"images\"].append({\n",
                "            \"id\": image_id,\n",
                "            \"file_name\": annotation['filename'],\n",
                "            \"width\": annotation['width'],\n",
                "            \"height\": annotation['height']\n",
                "        })\n",
                "        \n",
                "        for obj in annotation['objects']:\n",
                "            class_name = obj['class']\n",
                "            if class_name not in class_mapping:\n",
                "                continue\n",
                "            \n",
                "            xmin, ymin, xmax, ymax = obj['bbox']\n",
                "            width = xmax - xmin\n",
                "            height = ymax - ymin\n",
                "            \n",
                "            coco_data[\"annotations\"].append({\n",
                "                \"id\": annotation_id,\n",
                "                \"image_id\": image_id,\n",
                "                \"category_id\": class_mapping[class_name] + 1,\n",
                "                \"bbox\": [xmin, ymin, width, height],\n",
                "                \"area\": width * height,\n",
                "                \"iscrowd\": 0\n",
                "            })\n",
                "            annotation_id += 1\n",
                "    \n",
                "    return coco_data\n",
                "\n",
                "print(\"‚úÖ Data preparation functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Prepare Dataset (Pascal VOC ‚Üí COCO)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"PREPARING DATASET (COCO FORMAT)\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Create output directories\n",
                "for split in ['train', 'val', 'test']:\n",
                "    (OUTPUT_DIR / split).mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Create class mapping\n",
                "class_mapping = {name: idx for idx, name in enumerate(CLASSES)}\n",
                "\n",
                "# Get all image files\n",
                "image_files = list(RAW_DATA_DIR.glob(\"*.jpg\"))\n",
                "print(f\"\\nFound {len(image_files)} images\")\n",
                "\n",
                "# Parse annotations\n",
                "print(\"\\nParsing annotations...\")\n",
                "all_annotations = []\n",
                "valid_image_files = []\n",
                "\n",
                "for img_file in tqdm(image_files):\n",
                "    xml_file = img_file.with_suffix('.xml')\n",
                "    \n",
                "    if not xml_file.exists():\n",
                "        continue\n",
                "    \n",
                "    try:\n",
                "        annotation = parse_pascal_voc(xml_file)\n",
                "        if annotation['objects']:\n",
                "            all_annotations.append(annotation)\n",
                "            valid_image_files.append(img_file)\n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")\n",
                "        continue\n",
                "\n",
                "print(f\"Valid images: {len(valid_image_files)}\")\n",
                "\n",
                "# Split dataset\n",
                "train_idx, temp_idx = train_test_split(\n",
                "    range(len(valid_image_files)), \n",
                "    train_size=TRAIN_SPLIT, \n",
                "    random_state=42\n",
                ")\n",
                "val_idx, test_idx = train_test_split(\n",
                "    temp_idx,\n",
                "    train_size=VAL_SPLIT / (VAL_SPLIT + TEST_SPLIT),\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "splits = {\n",
                "    'train': train_idx,\n",
                "    'val': val_idx,\n",
                "    'test': test_idx\n",
                "}\n",
                "\n",
                "print(f\"\\nTrain: {len(train_idx)}\")\n",
                "print(f\"Val: {len(val_idx)}\")\n",
                "print(f\"Test: {len(test_idx)}\")\n",
                "\n",
                "# Process each split\n",
                "split_paths = {}\n",
                "\n",
                "for split_name, indices in splits.items():\n",
                "    print(f\"\\nProcessing {split_name}...\")\n",
                "    \n",
                "    split_annotations = [all_annotations[i] for i in indices]\n",
                "    split_images = [valid_image_files[i] for i in indices]\n",
                "    \n",
                "    coco_data = convert_to_coco_format(split_annotations, class_mapping)\n",
                "    \n",
                "    for img_file in tqdm(split_images, desc=f\"Copying {split_name}\"):\n",
                "        dst_img = OUTPUT_DIR / split_name / img_file.name\n",
                "        shutil.copy(img_file, dst_img)\n",
                "    \n",
                "    coco_json_path = OUTPUT_DIR / split_name / \"annotations.json\"\n",
                "    with open(coco_json_path, 'w') as f:\n",
                "        json.dump(coco_data, f, indent=2)\n",
                "    \n",
                "    split_paths[split_name] = {\n",
                "        'images': str(OUTPUT_DIR / split_name),\n",
                "        'annotations': str(coco_json_path)\n",
                "    }\n",
                "    \n",
                "    print(f\"{split_name.upper()}: {len(coco_data['images'])} images, {len(coco_data['annotations'])} objects\")\n",
                "\n",
                "print(\"\\n‚úÖ Dataset preparation complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Load Data for Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading training data...\")\n",
                "train_data = object_detector.Dataset.from_coco_folder(\n",
                "    split_paths['train']['images'],\n",
                "    annotations_json_path=split_paths['train']['annotations']\n",
                ")\n",
                "\n",
                "print(\"Loading validation data...\")\n",
                "val_data = object_detector.Dataset.from_coco_folder(\n",
                "    split_paths['val']['images'],\n",
                "    annotations_json_path=split_paths['val']['annotations']\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Data loaded successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Train Model\n",
                "\n",
                "**This will take 2-4 hours with GPU**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"TRAINING MEDIAPIPE OBJECT DETECTION MODEL\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Create model specification\n",
                "spec = object_detector.SupportedModels.get(MODEL_SPEC)\n",
                "\n",
                "# Configure hyperparameters\n",
                "hparams = object_detector.HParams(\n",
                "    learning_rate=0.3,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    epochs=EPOCHS,\n",
                "    cosine_decay_epochs=EPOCHS,\n",
                "    cosine_decay_alpha=1.0\n",
                ")\n",
                "\n",
                "# Train model\n",
                "print(f\"\\nModel: {MODEL_SPEC}\")\n",
                "print(f\"Epochs: {EPOCHS}\")\n",
                "print(f\"Batch size: {BATCH_SIZE}\")\n",
                "print(\"\\nStarting training...\\n\")\n",
                "\n",
                "model = object_detector.ObjectDetector.create(\n",
                "    train_data=train_data,\n",
                "    validation_data=val_data,\n",
                "    model_spec=spec,\n",
                "    hparams=hparams,\n",
                "    do_train=True\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Evaluate Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"EVALUATING MODEL\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "loss, coco_metrics = model.evaluate(val_data, batch_size=BATCH_SIZE)\n",
                "\n",
                "print(f\"\\nValidation Loss: {loss:.4f}\")\n",
                "print(f\"COCO mAP: {coco_metrics}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîü Export Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"EXPORTING MODEL\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Create output directory\n",
                "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Export to TFLite\n",
                "tflite_path = MODEL_OUTPUT_DIR / 'gym_apparatus_detector.tflite'\n",
                "model.export_model(str(tflite_path))\n",
                "\n",
                "print(f\"\\n‚úÖ Model exported to: {tflite_path}\")\n",
                "\n",
                "# Save label map\n",
                "label_map_path = MODEL_OUTPUT_DIR / 'labels.txt'\n",
                "with open(label_map_path, 'w') as f:\n",
                "    for class_name in CLASSES:\n",
                "        f.write(f\"{class_name}\\n\")\n",
                "\n",
                "print(f\"Label map saved to: {label_map_path}\")\n",
                "\n",
                "# Save metadata\n",
                "metadata = {\n",
                "    'model_spec': MODEL_SPEC,\n",
                "    'epochs': EPOCHS,\n",
                "    'batch_size': BATCH_SIZE,\n",
                "    'classes': CLASSES,\n",
                "    'num_classes': len(CLASSES),\n",
                "    'trained_on': datetime.now().isoformat(),\n",
                "    'validation_loss': float(loss),\n",
                "    'coco_metrics': str(coco_metrics)\n",
                "}\n",
                "\n",
                "metadata_path = MODEL_OUTPUT_DIR / 'metadata.json'\n",
                "with open(metadata_path, 'w') as f:\n",
                "    json.dump(metadata, f, indent=2)\n",
                "\n",
                "print(f\"Metadata saved to: {metadata_path}\")\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"TRAINING COMPLETE!\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì• Download Trained Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "# Download model\n",
                "files.download(str(tflite_path))\n",
                "files.download(str(label_map_path))\n",
                "files.download(str(metadata_path))\n",
                "\n",
                "print(\"‚úÖ Files downloaded!\")\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"1. Copy gym_apparatus_detector.tflite to your project\")\n",
                "print(\"2. Rename to gym_apparatus_custom.tflite\")\n",
                "print(\"3. Place in model_service/models/\")\n",
                "print(\"4. Restart your API server\")\n",
                "print(\"5. Test on gymnastics videos!\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
